{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import mdpy as mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_series(A, n):\n",
    "    mdp.is_square(A)\n",
    "    ret = np.zeros_like(A)\n",
    "    mat = np.eye(len(A))\n",
    "    for i in range(n):\n",
    "        ret += mat\n",
    "        mat = mat @ A\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "Here we define an MDP and solve it analytically, computing both the expected return (i.e., the value function `v_pi`) and its variance (`v_var`, using Sobel's approach) for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "per-state variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# MDP solved analytically\n",
    "ns = 6\n",
    "I = np.eye(ns)\n",
    "\n",
    "# Probability of transitioning from state s_i --> s_j = P[i,j]\n",
    "P = np.diag(np.ones(ns-1), 1) * 0.5\n",
    "P[:,0] = 0.5\n",
    "P[-1, 0] = 1\n",
    "\n",
    "# Expected reward for transitioning from s_i --> s_j = R[i,j]\n",
    "R = np.zeros((ns, ns))\n",
    "# -1 Reward for non-terminal transitions\n",
    "R[:,:] = -1\n",
    "# Reaching edge has zero reward\n",
    "R[-2, -1] = 0\n",
    "# Transitions from terminal state have zero reward\n",
    "R[-1,:] = 0\n",
    "r = np.sum(P*R, axis=1)\n",
    "\n",
    "# State-dependent discount\n",
    "gvec = np.ones(ns)*0.9\n",
    "gvec[0] = 0\n",
    "G = np.diag(gvec)\n",
    "\n",
    "# State-dependent bootstrapping\n",
    "lvec = np.ones(ns)*0.0\n",
    "L = np.diag(lvec)\n",
    "\n",
    "# Value function (expected Monte Carlo return)\n",
    "v_pi = pinv(I - P @ G) @ r\n",
    "\n",
    "# Compute stationary distribution for transition matrix\n",
    "d_pi = mdp.stationary(P)\n",
    "D = np.diag(d_pi)\n",
    "\n",
    "\n",
    "# From Sobel, setting up variance Bellman equation\n",
    "T = -v_pi**2\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        T[i] += P[i,j] * (R[i,j] + gvec[j]*v_pi[j])**2\n",
    "\n",
    "# Alternatively,\n",
    "# T = np.sum(P * (R + G @ v_pi)**2, axis=1) - v_pi**2\n",
    "        \n",
    "# Solve Bellman equation for variance of return\n",
    "v_var = pinv(I - P @ G @ G) @ T \n",
    "\n",
    "# print(T)\n",
    "print('v_pi:\\n', v_pi)\n",
    "print('per-state variance:\\n', v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying It Out Empirically\n",
    "\n",
    "Empirically checking via simulation is useful for finding errors.\n",
    "The code for simulating the MDP is somewhat inefficient but simple to write and debug.\n",
    "\n",
    "We use the same MDP as above. \n",
    "\n",
    "Note that the use of state-dependent γ allows for an effectively episodic problem without requiring us to actually break the trajectory into episodes.\n",
    "\n",
    "The trajectory is kept as a list of dictionaries that record all relevant data for each time step.\n",
    "We then compute additional quantities, the *return* and the *squared return*, taking the expression for the latter from the VTD paper (White & White)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_return(history):\n",
    "    ret = []\n",
    "    g = 0\n",
    "    for step in reversed(history):\n",
    "        g *= step['gm']\n",
    "        g += step['r']\n",
    "        ret.append({'g': g, **step})\n",
    "    return list(reversed(ret))\n",
    "\n",
    "def compute_squared_return(history):\n",
    "    ret = []\n",
    "    g_sq = 0\n",
    "    g_next = 0\n",
    "    for step in reversed(history):\n",
    "        g_sq *= step['gm']**2\n",
    "        g_sq += step['r']**2 + 2*step['gm']*step['r']*g_next\n",
    "        ret.append({'g_sq': g_sq, **step})\n",
    "        g_next = step['g']\n",
    "    return list(reversed(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate our MDP\n",
    "num_steps = 10000\n",
    "\n",
    "states = np.arange(ns)\n",
    "features = [i for i in np.eye(ns)]\n",
    "\n",
    "# Initial state\n",
    "s0  = 0\n",
    "s   = s0\n",
    "x   = features[s]\n",
    "\n",
    "history = []\n",
    "for i in range(num_steps):\n",
    "    p_next = x @ P\n",
    "    sp = np.random.choice(states, p=p_next)\n",
    "    xp = features[sp]\n",
    "    gm = gvec[sp]\n",
    "    reward = R[s, sp]\n",
    "    \n",
    "    history.append({'s': s, 'sp': sp, 'gm': gm, 'r': reward})\n",
    "    \n",
    "    # Next iteration\n",
    "    s = sp\n",
    "    x = xp.copy()\n",
    "    \n",
    "# Augment the history with the return at each timestep\n",
    "history = compute_return(history)\n",
    "history = compute_squared_return(history)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>g_sq</th>\n",
       "      <th>gm</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.439</td>\n",
       "      <td>11.826721</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.710</td>\n",
       "      <td>7.344100</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.900</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       g       g_sq   gm    r  s  sp\n",
       "0 -3.439  11.826721  0.9 -1.0  0   1\n",
       "1 -2.710   7.344100  0.9 -1.0  1   2\n",
       "2 -1.900   3.610000  0.9 -1.0  2   3\n",
       "3 -1.000   1.000000  0.9 -1.0  3   4\n",
       "4  0.000   0.000000  0.9  0.0  4   5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>s</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g_sq</th>\n",
       "      <td>4.008053</td>\n",
       "      <td>3.565755</td>\n",
       "      <td>2.745723</td>\n",
       "      <td>1.595544</td>\n",
       "      <td>0.455657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>-1.776638</td>\n",
       "      <td>-1.713232</td>\n",
       "      <td>-1.547019</td>\n",
       "      <td>-1.205360</td>\n",
       "      <td>-0.455657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "s            0         1         2         3         4    5\n",
       "g_sq  4.008053  3.565755  2.745723  1.595544  0.455657  0.0\n",
       "g    -1.776638 -1.713232 -1.547019 -1.205360 -0.455657  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check expected squared return\n",
    "grouped = df.groupby('s')\n",
    "g_sq = np.array(grouped.aggregate({'g_sq': np.mean}))\n",
    "\n",
    "# Display results\n",
    "grouped.aggregate({'g':np.mean, 'g_sq': np.mean}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance via E[G^2] - E[G]^2:\n",
      " [ 0.8516  0.6306  0.3525  0.1427  0.248   0.    ]\n",
      "Analytical variance (Sobel):\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Variance via squared-return minus expected return squared\n",
    "grouped = df.groupby('s')\n",
    "a = np.array(grouped.aggregate({'g': np.mean})**2)\n",
    "b = np.array(grouped.aggregate({'g_sq': np.mean}))\n",
    "print(\"Variance via E[G^2] - E[G]^2:\\n\", np.ravel(b - a))\n",
    "print(\"Analytical variance (Sobel):\\n\", v_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance (via numpy.var):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>s</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.85178</td>\n",
       "      <td>0.63084</td>\n",
       "      <td>0.352726</td>\n",
       "      <td>0.14287</td>\n",
       "      <td>0.248795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "s        0        1         2        3         4    5\n",
       "g  0.85178  0.63084  0.352726  0.14287  0.248795  0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with variance computed directly\n",
    "print(\"Variance (via numpy.var):\")\n",
    "grouped = df.groupby('s')\n",
    "grouped.aggregate({'g': np.var}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we our formulas were accurate, although more simulations could further improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Moment of the Return\n",
    "\n",
    "An alternative way to compute the variance analytically is by defining and solving a Bellman equation for the second moment of the return and using the fact that $\\operatorname{Var}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$.\n",
    "\n",
    "In *A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning*, White & White provide just such a Bellman equation, which we use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second moment of return:\n",
      " [ 3.9533  3.5187  2.7718  1.6525  0.5     0.    ]\n",
      "Estimated variance via second moment of return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Using the VTD paper to calculate second moments of the return.\n",
    "# Note that here we are using the most accurate values for everything\n",
    "# in order to check the equations.\n",
    "Pbar = np.zeros((ns, ns))\n",
    "Rbar = np.zeros((ns,ns))\n",
    "rbar = np.zeros(ns)\n",
    "\n",
    "# Specify parameters\n",
    "lvec = np.ones(ns)\n",
    "\n",
    "# Calculate R-bar transition matrix\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Rbar[i,j] = R[i,j]**2 + 2*gvec[j]*lvec[j]*R[i,j]*v_pi[j]\n",
    "\n",
    "# Calculate r-bar vector\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        rbar[i] += P[i,j]*Rbar[i,j]\n",
    "\n",
    "# Calculate P-bar\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Pbar[i,j] += P[i,j]*(gvec[j]**2)*(lvec[j]**2)\n",
    "        \n",
    "        \n",
    "# Calculate second moment of return\n",
    "r_second = pinv(I - Pbar) @ rbar\n",
    "\n",
    "# Print the results\n",
    "print(\"Second moment of return:\\n\", r_second)\n",
    "print(\"Estimated variance via second moment of return:\\n\", r_second - v_pi**2)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "\n",
    "\n",
    "# An alternative approach, which is somewhat more concise\n",
    "# Second moment of return\n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "The two methods produce the same result, as expected.\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "We calculated things as carefully as we can (and with as complete information as possible) to avoid mis-specifying things.\n",
    "Of note is that in a few cases whether the expected reward *matrix* vs. expected reward *vector* is used substantially affects the calculations.\n",
    "So while there are less verbose ways to solve the problem, the following is perhaps a little cleaner to examine and debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Moment of δ-return\n",
    "\n",
    "Using the equations in the White & White paper, we can calculate the second moment of the δ-return.\n",
    "The δ-return is the discounted sum of future TD-errors, which we can define via: \n",
    "\n",
    "$$\n",
    "G^{\\delta, \\lambda}_{t} = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "$$\n",
    "\n",
    "Note that the δ-return also encodes the bias (difference between the expected λ-return for each state and the approximate value function).\n",
    "\n",
    "$$G^{\\lambda}_{t} - \\hat{v}(S_t) = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}$$\n",
    "\n",
    "This means that for $\\lambda = 1$ we have a way of computing the bias with respect to the Monte Carlo return:\n",
    "\n",
    "$$\n",
    "G^{\\lambda=1}_{t} - \\hat{v}(S_t) = v_{\\pi} - \\hat{v} \n",
    "$$\n",
    "\n",
    "Disregarding the values of λ used in the approximation process for $\\hat{v}$, we can choose alternative values to get the bias with respect to a particular λ-return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Exact Value Function\n",
    "\n",
    "We check that our algorithm works by computing the second moment of the δ-return for when the approximate and 'true' value functions are identical.\n",
    "The expected TD-error for each state should be zero, as should the bias (which should be equal to the δ-return).\n",
    "\n",
    "The second moment of the δ-return may be nonzero.\n",
    "In fact, it should be equal to both the $\\delta^2$-return and the variance, and since the bias is zero, the mean squared-error as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "bias:\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      "δ-return:\n",
      " [ 0. -0.  0.  0.  0.  0.]\n",
      "δ^2-return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Second moment 'reward' (r-bar):\n",
      " [ 0.5839  0.4873  0.3039  0.0506  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Expected squared error:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function is identical to true value function\n",
    "v_hat = v_pi\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment expected 'reward' (r-bar):\\n\", dd)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inaccurate Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Approximate value function\n",
    "# v_hat = v_pi #+ 0.1*np.arange(len(v_hat))\n",
    "v_hat = v_pi + [0, -0.1, 0.5, 0, 0.0, 0]\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# Second moment of δ-return \n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)\n",
    "\n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment 'reward' (r-bar):\\n\", dd)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance of δ-return\n",
    "\n",
    "Given the popularity of the movie *Inception*, it behooves us to go deeper.\n",
    "\n",
    "On a more serious note, I am not really sure how to interpret this but kept it in anyways because I wish to avoid having to redo it if it turns out to be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "[ 0.6662  0.5396  0.3654  0.1519  0.25    0.    ]\n",
      "[ 0.175   0.0956 -0.     -0.     -0.     -0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "# v_hat = v_pi + 0.1*np.arange(len(v_hat))\n",
    "v_hat = v_pi + [0, -0.1, 0.5, 0, 0.0, 0]\n",
    "\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected per-state TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Sobel-like method for variance of δ-return \n",
    "T_d = -δ**2\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        T_d[i] += P[i,j] * (Δ[i,j] + gvec[j]*δ[j])**2\n",
    "\n",
    "# Alternatively,\n",
    "# T_d = np.sum(P * (Δ + G @ δ)**2, axis=1) - δ**2\n",
    "\n",
    "# Calculating variance of δ-return\n",
    "dd = pinv(I - P @ G @ G) @ T_d\n",
    "\n",
    "print(v_var)\n",
    "print(dd)\n",
    "print(v_var - dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
