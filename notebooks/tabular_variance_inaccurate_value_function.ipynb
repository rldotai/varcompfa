{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import mdpy as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We provide code for a few different ways of estimating the variance and second moment of cumulants in a finite MDP.\n",
    "\n",
    "We apply this code to analyzing some cumulants in particular: the reward, the TD-error ($\\delta$), and the squared TD-error ($\\delta^2$).\n",
    "\n",
    "We show that in the setting where the estimate of the value function is exact ($\\hat{v} = v_{\\pi}$), the $\\delta^2$-return (sum of discounted squared TD-errors) gives the variance (and trivially the mean squared error.\n",
    "\n",
    "We show also that the second moment of the $\\delta$-return (discounted sum of TD-errors$) gives the mean squared error even in settings where the approximate value function is perturbed from the true value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "Here we define an MDP and solve it analytically, computing both the expected return (i.e., the value function `v_pi`) and its variance (`v_var`, using Sobel's approach) for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "per-state variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# MDP solved analytically\n",
    "ns = 6\n",
    "I = np.eye(ns)\n",
    "\n",
    "# Probability of transitioning from state s_i --> s_j = P[i,j]\n",
    "P = np.diag(np.ones(ns-1), 1) * 0.5\n",
    "P[:,0] = 0.5\n",
    "P[-1, 0] = 1\n",
    "\n",
    "# Expected reward for transitioning from s_i --> s_j = R[i,j]\n",
    "R = np.zeros((ns, ns))\n",
    "# -1 Reward for non-terminal transitions\n",
    "R[:,:] = -1\n",
    "# Reaching edge has zero reward\n",
    "R[-2, -1] = 0\n",
    "# Transitions from terminal state have zero reward\n",
    "R[-1,:] = 0\n",
    "r = np.sum(P*R, axis=1)\n",
    "\n",
    "# State-dependent discount\n",
    "gvec = np.ones(ns)*0.9\n",
    "gvec[0] = 0\n",
    "G = np.diag(gvec)\n",
    "\n",
    "# State-dependent bootstrapping\n",
    "lvec = np.ones(ns)*0.0\n",
    "L = np.diag(lvec)\n",
    "\n",
    "# Value function (expected Monte Carlo return)\n",
    "v_pi = pinv(I - P @ G) @ r\n",
    "\n",
    "# Compute stationary distribution for transition matrix\n",
    "d_pi = mdp.stationary(P)\n",
    "D = np.diag(d_pi)\n",
    "\n",
    "\n",
    "# From Sobel, setting up variance Bellman equation\n",
    "T = -v_pi**2\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        T[i] += P[i,j] * (R[i,j] + gvec[j]*v_pi[j])**2\n",
    "\n",
    "# Alternatively,\n",
    "# T = np.sum(P * (R + G @ v_pi)**2, axis=1) - v_pi**2\n",
    "        \n",
    "# Solve Bellman equation for variance of return\n",
    "v_var = pinv(I - P @ G @ G) @ T \n",
    "\n",
    "# print(T)\n",
    "print('v_pi:\\n', v_pi)\n",
    "print('per-state variance:\\n', v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying It Out Empirically\n",
    "\n",
    "Empirically checking via simulation is useful for finding errors.\n",
    "The code for simulating the MDP is somewhat inefficient but simple to write and debug.\n",
    "\n",
    "We use the same MDP as above. \n",
    "\n",
    "Note that the use of state-dependent γ allows for an effectively episodic problem without requiring us to actually break the trajectory into episodes.\n",
    "\n",
    "The trajectory is kept as a list of dictionaries that record all relevant data for each time step.\n",
    "We then compute additional quantities, the *return* and the *squared return*, taking the expression for the latter from the VTD paper (White & White)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_return(history):\n",
    "    ret = []\n",
    "    g = 0\n",
    "    for step in reversed(history):\n",
    "        g *= step['gm']\n",
    "        g += step['r']\n",
    "        ret.append({'g': g, **step})\n",
    "    return list(reversed(ret))\n",
    "\n",
    "def compute_squared_return(history):\n",
    "    ret = []\n",
    "    g_sq = 0\n",
    "    g_next = 0\n",
    "    for step in reversed(history):\n",
    "        g_sq *= step['gm']**2\n",
    "        g_sq += step['r']**2 + 2*step['gm']*step['r']*g_next\n",
    "        ret.append({'g_sq': g_sq, **step})\n",
    "        g_next = step['g']\n",
    "    return list(reversed(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate our MDP\n",
    "num_steps = 10000\n",
    "\n",
    "states = np.arange(ns)\n",
    "features = [i for i in np.eye(ns)]\n",
    "\n",
    "# Initial state\n",
    "s0  = 0\n",
    "s   = s0\n",
    "x   = features[s]\n",
    "\n",
    "history = []\n",
    "for i in range(num_steps):\n",
    "    p_next = x @ P\n",
    "    sp = np.random.choice(states, p=p_next)\n",
    "    xp = features[sp]\n",
    "    gm = gvec[sp]\n",
    "    reward = R[s, sp]\n",
    "    \n",
    "    history.append({'s': s, 'sp': sp, 'gm': gm, 'r': reward})\n",
    "    \n",
    "    # Next iteration\n",
    "    s = sp\n",
    "    x = xp.copy()\n",
    "    \n",
    "# Augment the history with the return at each timestep\n",
    "history = compute_return(history)\n",
    "history = compute_squared_return(history)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>g_sq</th>\n",
       "      <th>gm</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.439</td>\n",
       "      <td>11.826721</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.710</td>\n",
       "      <td>7.344100</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.900</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.710</td>\n",
       "      <td>7.344100</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       g       g_sq   gm    r  s  sp\n",
       "0 -3.439  11.826721  0.9 -1.0  0   1\n",
       "1 -2.710   7.344100  0.9 -1.0  1   2\n",
       "2 -1.900   3.610000  0.9 -1.0  2   3\n",
       "3 -1.000   1.000000  0.0 -1.0  3   0\n",
       "4 -2.710   7.344100  0.9 -1.0  0   1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>s</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g_sq</th>\n",
       "      <td>4.002009</td>\n",
       "      <td>3.509261</td>\n",
       "      <td>2.672358</td>\n",
       "      <td>1.616077</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>-1.778378</td>\n",
       "      <td>-1.701001</td>\n",
       "      <td>-1.522674</td>\n",
       "      <td>-1.212440</td>\n",
       "      <td>-0.477419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "s            0         1         2         3         4    5\n",
       "g_sq  4.002009  3.509261  2.672358  1.616077  0.477419  0.0\n",
       "g    -1.778378 -1.701001 -1.522674 -1.212440 -0.477419  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check expected squared return\n",
    "grouped = df.groupby('s')\n",
    "g_sq = np.array(grouped.aggregate({'g_sq': np.mean}))\n",
    "\n",
    "# Display results\n",
    "grouped.aggregate({'g':np.mean, 'g_sq': np.mean}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance via E[G^2] - E[G]^2:\n",
      " [ 0.8394  0.6159  0.3538  0.1461  0.2495  0.    ]\n",
      "Analytical variance (Sobel):\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Variance via squared-return minus expected return squared\n",
    "grouped = df.groupby('s')\n",
    "a = np.array(grouped.aggregate({'g': np.mean})**2)\n",
    "b = np.array(grouped.aggregate({'g_sq': np.mean}))\n",
    "print(\"Variance via E[G^2] - E[G]^2:\\n\", np.ravel(b - a))\n",
    "print(\"Analytical variance (Sobel):\\n\", v_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance (via numpy.var):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>s</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.839548</td>\n",
       "      <td>0.616097</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.146299</td>\n",
       "      <td>0.250298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "s         0         1         2         3         4    5\n",
       "g  0.839548  0.616097  0.354093  0.146299  0.250298  0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with variance computed directly\n",
    "print(\"Variance (via numpy.var):\")\n",
    "grouped = df.groupby('s')\n",
    "grouped.aggregate({'g': np.var}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that we our formulas were accurate, although more simulations could further improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Moment of the Return\n",
    "\n",
    "An alternative way to compute the variance analytically is by defining and solving a Bellman equation for the second moment of the return and using the fact that $\\operatorname{Var}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$.\n",
    "\n",
    "In *A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning*, White & White provide just such a Bellman equation, which we use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second moment of return:\n",
      " [ 3.9533  3.5187  2.7718  1.6525  0.5     0.    ]\n",
      "Estimated variance via second moment of return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Using the VTD paper to calculate second moments of the return.\n",
    "# Note that here we are using the most accurate values for everything\n",
    "# in order to check the equations.\n",
    "Pbar = np.zeros((ns, ns))\n",
    "Rbar = np.zeros((ns,ns))\n",
    "rbar = np.zeros(ns)\n",
    "\n",
    "# Specify parameters\n",
    "lvec = np.ones(ns)\n",
    "\n",
    "# Calculate R-bar transition matrix\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Rbar[i,j] = R[i,j]**2 + 2*gvec[j]*lvec[j]*R[i,j]*v_pi[j]\n",
    "\n",
    "# Calculate r-bar vector\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        rbar[i] += P[i,j]*Rbar[i,j]\n",
    "\n",
    "# Calculate P-bar\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Pbar[i,j] += P[i,j]*(gvec[j]**2)*(lvec[j]**2)\n",
    "        \n",
    "        \n",
    "# Calculate second moment of return\n",
    "r_second = pinv(I - Pbar) @ rbar\n",
    "\n",
    "# Print the results\n",
    "print(\"Second moment of return:\\n\", r_second)\n",
    "print(\"Estimated variance via second moment of return:\\n\", r_second - v_pi**2)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "\n",
    "\n",
    "# An alternative approach, which is somewhat more concise\n",
    "# Second moment of return\n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "The two methods produce the same result, as expected.\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "We calculated things as carefully as we can (and with as complete information as possible) to avoid mis-specifying things.\n",
    "Of note is that in a few cases whether the expected reward *matrix* vs. expected reward *vector* is used substantially affects the calculations.\n",
    "So while there are less verbose ways to solve the problem, the following is perhaps a little cleaner to examine and debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Moment of δ-return\n",
    "\n",
    "Using the equations in the White & White paper, we can calculate the second moment of the δ-return.\n",
    "The δ-return is the discounted sum of future TD-errors, which we can define via: \n",
    "\n",
    "$$\n",
    "G^{\\delta, \\lambda}_{t} = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "$$\n",
    "\n",
    "Note that the δ-return also encodes the bias (difference between the expected λ-return for each state and the approximate value function).\n",
    "\n",
    "$$G^{\\lambda}_{t} - \\hat{v}(S_t) = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}$$\n",
    "\n",
    "This means that for $\\lambda = 1$ we have a way of computing the bias with respect to the Monte Carlo return:\n",
    "\n",
    "$$\n",
    "G^{\\lambda=1}_{t} - \\hat{v}(S_t) = v_{\\pi} - \\hat{v} \n",
    "$$\n",
    "\n",
    "Disregarding the values of λ used in the approximation process for $\\hat{v}$, we can choose alternative values to get the bias with respect to a particular λ-return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Exact Value Function\n",
    "\n",
    "We check that our algorithm works by computing the second moment of the δ-return for when the approximate and 'true' value functions are identical.\n",
    "The expected TD-error for each state should be zero, as should the bias (which should be equal to the δ-return).\n",
    "\n",
    "The second moment of the δ-return may be nonzero.\n",
    "In fact, it should be equal to both the $\\delta^2$-return and the variance, and since the bias is zero, the mean squared-error as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "bias:\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      "δ-return:\n",
      " [ 0. -0.  0.  0.  0.  0.]\n",
      "δ^2-return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Second moment expected 'reward' (r-bar):\n",
      " [ 0.5839  0.4873  0.3039  0.0506  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Expected squared error:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function is identical to true value function\n",
    "v_hat = v_pi\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment expected 'reward' (r-bar):\\n\", dd)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the expected results.\n",
    "\n",
    "Incidentally, this provides an alternative route for proving the efficacy of algorithms estimating the $\\delta^2$-return for computing variance in the tabular setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inaccurate Value Function\n",
    "\n",
    "We now examine what happens to the $\\delta$-return and its second moment (as well as the $\\delta^2$-return) in the case where the value function is no longer exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Perturbation, Single State\n",
    "\n",
    "We first examine the impact of a relatively large perturbation to a single state's value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7641 -1.6981 -1.0513 -1.225  -0.5    -0.    ]\n",
      "bias:\n",
      " [ 0.   0.  -0.5  0.   0.   0. ]\n",
      "δ-return:\n",
      " [ 0.  -0.  -0.5  0.   0.   0. ]\n",
      "δ^2-return:\n",
      " [ 0.796   0.5236  0.6154  0.1519  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8412  0.6353  0.6154  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Bias^2:\n",
      " [ 0.    0.    0.25  0.    0.    0.  ]\n",
      "Expected squared error:\n",
      " [ 0.8412  0.6353  0.6154  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "v_hat = v_pi + [0, 0.0, 0.5, 0, 0.0, 0]\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# Second moment of δ-return \n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)\n",
    "\n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Bias^2:\\n\", bias**2)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The bias is still given by the δ-return, and the $\\delta^2$-return still accurately gives the variance for states that do not bootstrap from the state where the error was introduced.\n",
    "\n",
    "As hypothesized, the second moment of the δ-return now gives the MSE rather than the variance.\n",
    "\n",
    "The $\\delta^2$-return for the state whose value was perturbed also gives the MSE (as do other states which do not bootstrap from it).\n",
    "For the states that *do* bootstrap from the perturbation in the value function, we note that they underestimate both the MSE and the variance (although they would *overestimate* if the sign of the variance were to be changed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Small Perturbation, Single State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7641 -1.6981 -1.4513 -1.225  -0.5    -0.    ]\n",
      "bias:\n",
      " [ 0.   0.  -0.1  0.   0.   0. ]\n",
      "δ-return:\n",
      " [ 0.   0.  -0.1  0.   0.   0. ]\n",
      "δ^2-return:\n",
      " [ 0.819   0.5805  0.3754  0.1519  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8412  0.6353  0.3754  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Bias^2:\n",
      " [ 0.    0.    0.01  0.    0.    0.  ]\n",
      "Expected squared error:\n",
      " [ 0.8412  0.6353  0.3754  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "v_hat = v_pi + [0, 0.0, 0.1, 0, 0.0, 0]\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# Second moment of δ-return \n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)\n",
    "\n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Bias^2:\\n\", bias**2)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see similar results as in the large-perturbation, but note that the difference between the $\\delta^2-return and the analytical variance actually seems to decrease the further 'away' we get from the perturbed state.\n",
    "\n",
    "The second moment of the $\\delta$-return continues to reflect the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0222, -0.0547,  0.01  ,  0.    ,  0.    , -0.    ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sq - v_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_second - ((v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Perturbations, Multiple States\n",
    "\n",
    "We now check a small uniform perturbation to the value function of all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.6641 -1.5981 -1.4513 -1.125  -0.4    -0.    ]\n",
      "bias:\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1  0. ]\n",
      "δ-return:\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1  0. ]\n",
      "δ^2-return:\n",
      " [ 0.746   0.5576  0.3163  0.1407  0.26    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8512  0.6453  0.3754  0.1619  0.26    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Bias^2:\n",
      " [ 0.01  0.01  0.01  0.01  0.01  0.  ]\n",
      "Expected squared error:\n",
      " [ 0.8512  0.6453  0.3754  0.1619  0.26    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "v_hat = v_pi + [0.1, 0.1, 0.1, 0.1, 0.1, 0]\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# Second moment of δ-return \n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)\n",
    "\n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Bias^2:\\n\", bias**2)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is still the same as the second moment of the $\\delta$-return.\n",
    "\n",
    "The $\\delta^2$ return has its errors compound as perturbed bootstraps bootstrap from perturbed states.\n",
    "The start state is off by almost a factor of ten with respect to its perturbation.\n",
    "Note that it is not accounting for the bias, it is just incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0952, -0.0777, -0.0491, -0.0111,  0.01  , -0.    ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sq - v_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1052, -0.0877, -0.0591, -0.0211,  0.    , -0.    ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sq - ((v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0.,  0.,  0.,  0., -0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_second - ((v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Perturbations of Alternating Sign\n",
    "\n",
    "We can check if having the perturbations be somewhat less correlated (a far more likely case in real-world settings) affects the various estimators under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.8641 -1.5981 -1.6513 -1.125  -0.6    -0.    ]\n",
      "bias:\n",
      " [ 0.1 -0.1  0.1 -0.1  0.1  0. ]\n",
      "δ-return:\n",
      " [ 0.1 -0.1  0.1 -0.1  0.1  0. ]\n",
      "δ^2-return:\n",
      " [ 0.829   0.7182  0.358   0.1992  0.26    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8512  0.6453  0.3754  0.1619  0.26    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Bias^2:\n",
      " [ 0.01  0.01  0.01  0.01  0.01  0.  ]\n",
      "Expected squared error:\n",
      " [ 0.8512  0.6453  0.3754  0.1619  0.26    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "v_hat = v_pi + [-0.1, 0.1, -0.1, 0.1, -0.1, 0]\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# Second moment of δ-return \n",
    "rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "vv = pinv(I - P @ G @ G)@(rr)\n",
    "\n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "print(\"Bias^2:\\n\", bias**2)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is still the same as the second moment of the $\\delta$-return.\n",
    "\n",
    "For the $\\delta^2$-return, the results are somewhat more mixed.\n",
    "The errors cancel in some places (state 3) and compound in others when we compare it with the variance.\n",
    "\n",
    "Overall it doesn't really seem to improve things, although a more carefully crafted set of perturbations could likely be found. One strategy might be to select improve feature selection to ensure that as many of the errors cancel each other out as possible.\n",
    "\n",
    "However, in general this suggests that some method of accounting for the cross-terms in the expansion is desirable, if we want to continue to use the $\\delta^2$-return in lieu of the approximation morass that is estimating the $\\delta$-return online under function approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0122,  0.0829, -0.0074,  0.0474,  0.01  ,  0.    ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sq - v_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0222,  0.0729, -0.0174,  0.0374,  0.    ,  0.    ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_sq - ((v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0.,  0.,  0.,  0., -0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_second - ((v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Notes\n",
    "\n",
    "While the above may seem appealing, there is still work to be done to get things into a more useful form.\n",
    "\n",
    "At the very least, it's a way of directly computing the MSE for a given value function, assuming the full MDP is known.\n",
    "This could be interesting to pursue, although I am not sure how practical the results would be.\n",
    "\n",
    "Optimizing the bias-variance trade-off via finding the minimum of the MSE using the second moment of the $\\delta$-return looks interesting, but requires more analysis to derive the appropriate equations.\n",
    "\n",
    "We note that under function approximation, the fixed point for the weights when approximating the $\\delta$-return is zero, meaning a different strategy might have to be employed if we were to actually use this.\n",
    "I can think of a few strategies for overcoming this (using multiple representations, or splitting the representations to make it possible to estimate the $\\delta$-return under LFA, perhaps a re-weighting method in the style of Emphatic TD, or choosing a more approximable target that still imparts useful information by modifying the bootstrapping/discount factors).\n",
    "\n",
    "Suggestions or comments are welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance of δ-return\n",
    "\n",
    "Given the popularity of the movie *Inception*, it behooves us to go deeper.\n",
    "\n",
    "On a more serious note, I am not really sure how to interpret this but kept it in anyways because I wish to avoid having to redo it if it turns out to be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "[ 0.6662  0.5396  0.3654  0.1519  0.25    0.    ]\n",
      "[ 0.175   0.0956 -0.     -0.     -0.     -0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Approximate value function\n",
    "# v_hat = v_pi + 0.1*np.arange(len(v_hat))\n",
    "v_hat = v_pi + [0, -0.1, 0.5, 0, 0.0, 0]\n",
    "\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected per-state TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Sobel-like method for variance of δ-return \n",
    "T_d = -δ**2\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        T_d[i] += P[i,j] * (Δ[i,j] + gvec[j]*δ[j])**2\n",
    "\n",
    "# Alternatively,\n",
    "# T_d = np.sum(P * (Δ + G @ δ)**2, axis=1) - δ**2\n",
    "\n",
    "# Calculating variance of δ-return\n",
    "dd = pinv(I - P @ G @ G) @ T_d\n",
    "\n",
    "print(v_var)\n",
    "print(dd)\n",
    "print(v_var - dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
