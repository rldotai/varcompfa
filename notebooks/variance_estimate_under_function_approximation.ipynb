{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import mdpy as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We provide code for a few different ways of estimating variance and second moments of various cumulants in a discrete MDP.\n",
    "In this notebook we focus on the case where linear function approximation is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "We define and solve an MDP for its value function and its variance (using Sobel's method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_pi:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "per-state variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# MDP solved analytically\n",
    "ns = 6\n",
    "I = np.eye(ns)\n",
    "\n",
    "# Probability of transitioning from state s_i --> s_j = P[i,j]\n",
    "P = np.diag(np.ones(ns-1), 1) * 0.5\n",
    "P[:,0] = 0.5\n",
    "P[-1, 0] = 1\n",
    "\n",
    "# Expected reward for transitioning from s_i --> s_j = R[i,j]\n",
    "R = np.zeros((ns, ns))\n",
    "# -1 Reward for non-terminal transitions\n",
    "R[:,:] = -1\n",
    "# Reaching edge has zero reward\n",
    "R[-2, -1] = 0\n",
    "# Transitions from terminal state have zero reward\n",
    "R[-1,:] = 0\n",
    "r = np.sum(P*R, axis=1)\n",
    "\n",
    "# State-dependent discount\n",
    "gvec = np.ones(ns)*0.9\n",
    "gvec[0] = 0\n",
    "G = np.diag(gvec)\n",
    "\n",
    "# State-dependent bootstrapping\n",
    "lvec = np.ones(ns)*0.0\n",
    "L = np.diag(lvec)\n",
    "\n",
    "# Value function (expected Monte Carlo return)\n",
    "v_pi = pinv(I - P @ G) @ r\n",
    "\n",
    "# Compute stationary distribution for transition matrix\n",
    "d_pi = mdp.stationary(P)\n",
    "D = np.diag(d_pi)\n",
    "\n",
    "\n",
    "# From Sobel, setting up variance Bellman equation\n",
    "T = -v_pi**2\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        T[i] += P[i,j] * (R[i,j] + gvec[j]*v_pi[j])**2\n",
    "\n",
    "# Alternatively,\n",
    "T = (P * (R + G @ v_pi)**2) @ np.ones(ns) - v_pi**2\n",
    "        \n",
    "# Solve Bellman equation for variance of return\n",
    "v_var = pinv(I - P @ G @ G) @ T \n",
    "\n",
    "print('v_pi:\\n', v_pi)\n",
    "print('per-state variance:\\n', v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Moment of Return\n",
    "\n",
    "We calculate the second moment of the return following the approach in the VTD paper (White & White, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second moment of return:\n",
      " [ 3.9533  3.5187  2.7718  1.6525  0.5     0.    ]\n",
      "Estimated variance via second moment of return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Sobel variance:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Using the VTD paper to calculate second moments of the return.\n",
    "# Note that here we are using the most accurate values for everything\n",
    "# in order to check the equations.\n",
    "Pbar = np.zeros((ns, ns))\n",
    "Rbar = np.zeros((ns,ns))\n",
    "rbar = np.zeros(ns)\n",
    "\n",
    "# Specify parameters\n",
    "lvec = np.ones(ns)\n",
    "\n",
    "# Calculate R-bar transition matrix\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Rbar[i,j] = R[i,j]**2 + 2*gvec[j]*lvec[j]*R[i,j]*v_pi[j]\n",
    "\n",
    "# Calculate r-bar vector\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        rbar[i] += P[i,j]*Rbar[i,j]\n",
    "\n",
    "# Calculate P-bar\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Pbar[i,j] += P[i,j]*(gvec[j]**2)*(lvec[j]**2)\n",
    "        \n",
    "        \n",
    "# Calculate second moment of return\n",
    "r_second = pinv(I - Pbar) @ rbar\n",
    "\n",
    "# Print the results\n",
    "print(\"Second moment of return:\\n\", r_second)\n",
    "print(\"Estimated variance via second moment of return:\\n\", r_second - v_pi**2)\n",
    "print(\"Sobel variance:\\n\", v_var)\n",
    "\n",
    "\n",
    "# An alternative approach, which is somewhat more concise\n",
    "# Second moment of return\n",
    "# rr = (P*R**2) @ np.ones(ns) + (2*P @ G * R) @ v_pi\n",
    "# vv = pinv(I - P @ G @ G)@(rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Function Approximation \n",
    "\n",
    "Linear function approximation is an important test case when analyzing the utility of a learning algorithm.\n",
    "\n",
    "Here we use a relatively simple scheme where we take the binary representation of the state's number as its feature representation, with a bias unit.\n",
    "\n",
    "So state \\#2 becomes `[1, 0, 1, 0]` and state \\#3 becomes `[1, 0, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hacky way of computing binary representation of state number\n",
    "blst = [np.binary_repr(i, width=int(np.ceil(np.log2(ns)))) for i in range(ns)]\n",
    "# convert strings to array form while prepending a bias unit\n",
    "flst = [np.append(1, np.array([int(x) for x in s])) for s in blst]\n",
    "# feature matrix\n",
    "X = np.array(flst)\n",
    "# set terminal state features to zero vector\n",
    "X[-1] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [1, 0, 1, 0],\n",
       "       [1, 0, 1, 1],\n",
       "       [1, 1, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(λ) fixed point\n",
    "\n",
    "We compute the weights that TD(λ) converges to asymptotically, and the associated value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(λ) fixed point weights:\n",
      " [-1.7646  1.2646  0.2232  0.12  ]\n",
      "TD(λ) approximate value function:\n",
      " [-1.7646 -1.6445 -1.5414 -1.4214 -0.5     0.    ]\n",
      "v_pi - v_hat:\n",
      " [ 0.0005 -0.0535 -0.0098  0.1964 -0.     -0.    ]\n",
      "Error due to bias:\n",
      " 0.0415271656424\n"
     ]
    }
   ],
   "source": [
    "# Compute TD(λ) fixed point\n",
    "w_hat = pinv(X.T @ D @ pinv(I - P @ G @ L) @ (I - P @ G) @ X) @ X.T @ D @ pinv(I - P @ G @ L) @ r\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "print('TD(λ) fixed point weights:\\n', w_hat)\n",
    "print('TD(λ) approximate value function:\\n', v_hat)\n",
    "print('v_pi - v_hat:\\n', v_pi - v_hat)\n",
    "print('Error due to bias:\\n', np.sum((v_pi - v_hat)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares / Monte Carlo Approximation\n",
    "\n",
    "We can also calculate the fixed point for when $\\lambda=1$ and we are approximating the Monte Carlo return. \n",
    "\n",
    "This is also called the least-squares approximation, because we are essentially performing a weighted least-squares regression on the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-squares weights:\n",
      " [-1.7815  1.2815  0.2996  0.1181]\n",
      "Least-squares approximate value function:\n",
      " [-1.7815 -1.6634 -1.4819 -1.3638 -0.5     0.    ]\n",
      "v_pi - v_lsq:\n",
      " [ 0.0173 -0.0347 -0.0694  0.1388  0.     -0.    ]\n",
      "Error due to bias (least-squares):\n",
      " 0.0255740100645\n"
     ]
    }
   ],
   "source": [
    "# Compute least-squares / TD(1) fixed point\n",
    "w_lsq = pinv(X.T @ D @ X) @ X.T @ D @ pinv(I - P @ G) @ r\n",
    "v_lsq = X @ w_lsq\n",
    "\n",
    "print('Least-squares weights:\\n', w_lsq)\n",
    "print('Least-squares approximate value function:\\n', v_lsq)\n",
    "print('v_pi - v_lsq:\\n', v_pi - v_lsq)\n",
    "print('Error due to bias (least-squares):\\n', np.sum((v_pi - v_lsq)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Moment Calculation\n",
    "\n",
    "\n",
    "In a previous notebook analyzing the tabular case, we found that the second moment of the $\\delta$-return provides us with the mean squared error for a given function approximator, and that in some situations the $\\delta^2$-return is a good approximation to this quantity.\n",
    "\n",
    "## Remarks and Review\n",
    "\n",
    "Recall that the $\\delta$-return is defined as \n",
    "\n",
    "$$\n",
    "G^{\\delta, \\lambda}_{t} = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "$$\n",
    "\n",
    "Furthermore, we have the following identity for what the $\\delta$-return represents:\n",
    "\n",
    "$$\n",
    "G^{\\lambda}_{t} - \\hat{v}(S_t) = \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "$$\n",
    "\n",
    "So the second moment of the $\\delta$-return (setting $\\lambda=1$) gives us $\\mathbb{E}[(G_{t} - \\hat{v}(S_t))^2]$.\n",
    "Although a more careful analysis is required, it was enough to conjecture that this quantity is really the MSE, incorporating both the error introduced via bias in the value function and the variance in the MDP. \n",
    "\n",
    "The $\\delta^2$-return, defined as:\n",
    "\n",
    "$$\n",
    "G^{\\delta^{2}, \\lambda}_{t} = \\sum_{n=0}^{\\infty} \\delta_{t+n}^2 \\prod_{k=1}^{n-1} \\gamma_{t+k}^2 \\lambda_{t+k}^2\n",
    "$$\n",
    "\n",
    "Is well-defined, and likely more amenable to approximation via LFA, but except when the bias is small doesn't really capture either the variance or the bias.\n",
    "This is because it is missing the 'cross-terms' of the full second moment expansion.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(G^{\\lambda}_{t} - \\hat{v}(S_{t}))^2 \n",
    "&= \\Bigg( \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\\Bigg)^2\n",
    "\\\\\n",
    "&= \\sum_{n=0}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "\\Bigg(\n",
    "\\sum_{m=0}^{\\infty} \\delta_{t+m} \\prod_{j=1}^{m-1} \\gamma_{t+j} \\lambda_{t+j}\n",
    "\\Bigg)\n",
    "\\\\\n",
    "&= \\delta_{t}^2 \n",
    "+ \\delta_{t} \\Bigg(\n",
    "\\sum_{m=1}^{\\infty} \\delta_{t+m} \\prod_{j=1}^{m-1} \\gamma_{t+j} \\lambda_{t+j}\n",
    "\\Bigg)\n",
    "+ \\sum_{n=1}^{\\infty} \\delta_{t+n} \\prod_{k=1}^{n-1} \\gamma_{t+k} \\lambda_{t+k}\n",
    "\\Bigg(\n",
    "\\sum_{m=1}^{\\infty} \\delta_{t+m} \\prod_{j=1}^{m-1} \\gamma_{t+j} \\lambda_{t+j}\n",
    "\\Bigg)\n",
    "\\\\\n",
    "&= \\delta_{t}^{2} + (G^{\\lambda}_{t+1} - \\hat{v}(S_{t+1}))^2 \n",
    "+ \\underbrace{\\delta_{t} \\Bigg(\n",
    "\\sum_{m=1}^{\\infty} \\delta_{t+m} \\prod_{j=1}^{m-1} \\gamma_{t+j} \\lambda_{t+j}\n",
    "\\Bigg)}_{\\text{'cross terms'}}\n",
    "\\\\\n",
    "&= G_{t}^{\\delta^{2}, \\lambda} \n",
    "+ \\delta_{t} \\Bigg(\n",
    "\\sum_{m=1}^{\\infty} \\delta_{t+m} \\prod_{j=1}^{m-1} \\gamma_{t+j} \\lambda_{t+j}\n",
    "\\Bigg)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When the bias is zero, these cross terms disappear (because in expectation $\\delta_{t}$ is zero for all times $t$).\n",
    "When the bias is sufficiently small, the combination of discounting and randomness tends to diminish their contribution as well.\n",
    "As the bias gets larger, or if it is correlated (persistently over/underestimating the value function), the difference between the $\\delta^2$-return and the second moment of the $\\delta$-return grows rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results for LFA\n",
    "\n",
    "We check what happens when we use the value function derived under LFA with our simple representation.\n",
    "\n",
    "Note that this assumes we are able to estimate the $\\delta$-return, its second moment, and the $\\delta^2$-return exactly, which is likely not the case in the 'real-world' given that we had to make an approximation in order to get the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(λ) Fixed Point\n",
    "\n",
    "We first examine the various quantities using the value function that TD(λ) converges to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7646 -1.6445 -1.5414 -1.4214 -0.5     0.    ]\n",
      "bias:\n",
      " [ 0.0005 -0.0535 -0.0098  0.1964 -0.     -0.    ]\n",
      "δ-return:\n",
      " [ 0.0005 -0.0535 -0.0098  0.1964 -0.      0.    ]\n",
      "δ^2-return:\n",
      " [ 0.8254  0.6844  0.4959  0.1904  0.25    0.    ]\n",
      "Second moment expected 'reward' (r-bar):\n",
      " [ 0.5827  0.4901  0.2884  0.0892  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8412  0.6381  0.3655  0.1904  0.25    0.    ]\n",
      "Variance of return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Expected squared error:\n",
      " [ 0.8412  0.6381  0.3655  0.1904  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Compute TD(λ) fixed point\n",
    "w_hat = pinv(X.T @ D @ pinv(I - P @ G @ L) @ (I - P @ G) @ X) @ X.T @ D @ pinv(I - P @ G @ L) @ r\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "# Print summary of results\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Second moment expected 'reward' (r-bar):\\n\", dd)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Variance of return:\\n\", v_var)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-Squares Fixed Point\n",
    "\n",
    "We now check the results using the least-squares approximation as our value function.\n",
    "This will be more accurate (less biased) almost by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7815 -1.6634 -1.4819 -1.3638 -0.5     0.    ]\n",
      "bias:\n",
      " [ 0.0173 -0.0347 -0.0694  0.1388  0.     -0.    ]\n",
      "δ-return:\n",
      " [ 0.0173 -0.0347 -0.0694  0.1388  0.      0.    ]\n",
      "δ^2-return:\n",
      " [ 0.8175  0.6323  0.4633  0.1711  0.25    0.    ]\n",
      "Variance of return using v_hat:\n",
      " [ 0.7436  0.74    0.7007 -0.2074  0.25    0.    ]\n",
      "Second moment expected 'reward' (r-bar):\n",
      " [ 0.5837  0.4865  0.3009  0.0699  0.25    0.    ]\n",
      "Second moment of delta-return:\n",
      " [ 0.8415  0.6365  0.3702  0.1711  0.25    0.    ]\n",
      "Variance of return:\n",
      " [ 0.8412  0.6353  0.3654  0.1519  0.25    0.    ]\n",
      "Expected squared error:\n",
      " [ 0.8415  0.6365  0.3702  0.1711  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Compute TD(λ) fixed point\n",
    "w_hat = w_lsq\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "# Variance using Sobel's method and approximate value function\n",
    "T_hat = (P * (R + G @ v_hat)**2) @ np.ones(ns) - v_hat**2\n",
    "        \n",
    "# Solve Bellman equation for variance of return w/ approx value function\n",
    "var_hat = pinv(I - P @ G @ G) @ T_hat\n",
    "\n",
    "# Print summary of results\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Variance of return using v_hat:\\n\", var_hat)\n",
    "print(\"Second moment expected 'reward' (r-bar):\\n\", dd)\n",
    "print(\"Second moment of delta-return:\\n\", gd_second)\n",
    "print(\"Variance of return:\\n\", v_var)\n",
    "print(\"Expected squared error:\\n\", (v_pi - v_hat)**2 + v_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we approximate the δ-return with the same features?\n",
    "\n",
    "In short, the answer is 'no, not really'.\n",
    "This can be shown from some linear algebra, but in essence the problem is that if you were able to estimate how biased your predictor was with certainty, then you wouldn't need to be biased at all.\n",
    "\n",
    "We are still not computing these quantities online. \n",
    "The current approach is similar to a batch setting, where for some reason we have access to the full problem, but elect to approximate the value function and then also approximate these quantities as well.\n",
    "\n",
    "We use the value function from the TD(0) fixed point, and estimate the δ- and δ^2-returns using the TD(1) fixed point.\n",
    "\n",
    "This allows for us to arrive at a nonzero value function for the δ-return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7646 -1.6445 -1.5414 -1.4214 -0.5     0.    ]\n",
      "bias:\n",
      " [ 0.0005 -0.0535 -0.0098  0.1964 -0.     -0.    ]\n",
      "δ-return:\n",
      " [ 0.0005 -0.0535 -0.0098  0.1964 -0.      0.    ]\n",
      "δ^2-return:\n",
      " [ 0.8254  0.6844  0.4959  0.1904  0.25    0.    ]\n",
      "Approximate δ-return:\n",
      " [-0.0169 -0.0188  0.0596  0.0576 -0.      0.    ]\n",
      "Approximate δ^2-return:\n",
      " [ 0.8364  0.6625  0.4521  0.2781  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Compute TD(0) fixed point\n",
    "L     = np.eye(ns)*0\n",
    "A_inv = pinv(X.T @ D @ pinv(I - P @ G @ L) @ (I - P @ G) @ X) @ X.T @ D\n",
    "w_hat = A_inv @ pinv(I - P @ G @ L) @ r\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "################################################################################\n",
    "# LFA for δ-return, δ^2-return, and moments\n",
    "\n",
    "L = np.eye(ns)\n",
    "wd_hat = pinv(X.T @ D @ pinv(I - P @ G @ L) @ (I - P @ G) @ X) @ X.T @ D @ pinv(I - P @ G @ L) @ δ\n",
    "vd_hat = X @ wd_hat\n",
    "\n",
    "# This may not be right\n",
    "Gbar = G @ G\n",
    "Lbar = np.diag(np.ones(ns))\n",
    "A_sq_inv = pinv(X.T @ D @ pinv(I - P @ Gbar @ Lbar) @ (I - P @ Gbar) @ X) @ X.T @ D\n",
    "wdsq_hat = A_sq_inv @ pinv(I - P @ Gbar @ Lbar) @ δ_sq\n",
    "vdsq_hat = X @ wdsq_hat\n",
    "\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Approximate δ-return:\\n\", vd_hat)\n",
    "print(\"Approximate δ^2-return:\\n\", vdsq_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on Approximation Approach\n",
    "\n",
    "We note that it is not necessary to use the same bootstrapping parameter (λ) when approximating the various quantities of interest.\n",
    "\n",
    "Using a different $\\lambda$ *might* allow us to approximate the $\\delta$-return with the same representation, although intuitively it seems like it still won't be possible to capture everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ for v_hat:\n",
      " [ 1.  1.  1.  1.  1.  1.]\n",
      "λ for δ-return:\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      "λ for δ^2-return:\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      "v_π:\n",
      " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
      "v_hat:\n",
      " [-1.7815 -1.6634 -1.4819 -1.3638 -0.5     0.    ]\n",
      "bias:\n",
      " [ 0.0173 -0.0347 -0.0694  0.1388  0.     -0.    ]\n",
      "δ-return:\n",
      " [ 0.0173 -0.0347 -0.0694  0.1388  0.      0.    ]\n",
      "δ^2-return:\n",
      " [ 0.8175  0.6323  0.4633  0.1711  0.25    0.    ]\n",
      "Approximate δ-return:\n",
      " [ 0.0169  0.0188 -0.0596 -0.0576  0.      0.    ]\n",
      "Approximate δ^2-return:\n",
      " [ 0.5752  0.417   0.3387  0.1805  0.25    0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Compute TD(λ) fixed point\n",
    "L1    = np.eye(ns)*1\n",
    "A_inv = pinv(X.T @ D @ pinv(I - P @ G @ L1) @ (I - P @ G) @ X) @ X.T @ D\n",
    "w_hat = A_inv @ pinv(I - P @ G @ L1) @ r\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "# Bias of approximate value function\n",
    "bias = v_pi - v_hat\n",
    "\n",
    "# TD error matrix, for error given transition i-->j\n",
    "Δ = np.zeros_like(R)\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Δ[i,j] = (R[i,j] + gvec[j]*v_hat[j] - v_hat[i])\n",
    "\n",
    "# Expected TD-error\n",
    "δ = (P*Δ) @ np.ones(ns)\n",
    "        \n",
    "# Expected δ^2\n",
    "δ_sq = (P * Δ**2) @ np.ones(ns)\n",
    "        \n",
    "# δ-return\n",
    "gd = pinv(I - P @ G) @ δ\n",
    "\n",
    "# δ^2-return\n",
    "gd_sq = pinv(I - P @ G @ G) @ δ_sq\n",
    "\n",
    "# Second moment of δ-return\n",
    "dd = (P * Δ**2) @ np.ones(ns) + (2*P @ G * Δ) @ gd\n",
    "gd_second = pinv(I - P @ G @ G)@(dd)\n",
    "\n",
    "################################################################################\n",
    "# LFA for δ-return, δ^2-return, and moments\n",
    "# with potentially distinct lambda\n",
    "\n",
    "L2 = np.eye(ns)*0\n",
    "\n",
    "wd_hat = pinv(X.T @ D @ pinv(I - P @ G @ L2) @ (I - P @ G) @ X) @ X.T @ D @ pinv(I - P @ G @ L2) @ δ\n",
    "vd_hat = X @ wd_hat\n",
    "\n",
    "# This may not be right\n",
    "L3   = np.eye(ns)*0\n",
    "Gbar = G @ G @ L3 @ L3\n",
    "Lbar = np.eye(ns)\n",
    "A_sq_inv = pinv(X.T @ D @ pinv(I - P @ Gbar @ Lbar) @ (I - P @ Gbar) @ X) @ X.T @ D\n",
    "wdsq_hat = A_sq_inv @ pinv(I - P @ Gbar @ Lbar) @ δ_sq\n",
    "vdsq_hat = X @ wdsq_hat\n",
    "\n",
    "print(\"λ for v_hat:\\n\", np.diag(L1))\n",
    "print(\"λ for δ-return:\\n\", np.diag(L2))\n",
    "print(\"λ for δ^2-return:\\n\", np.diag(L3))\n",
    "print(\"v_π:\\n\", v_pi)\n",
    "print(\"v_hat:\\n\", v_hat)\n",
    "print(\"bias:\\n\", v_pi-v_hat)\n",
    "print(\"δ-return:\\n\", gd)\n",
    "print(\"δ^2-return:\\n\", gd_sq)\n",
    "print(\"Approximate δ-return:\\n\", vd_hat)\n",
    "print(\"Approximate δ^2-return:\\n\", vdsq_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Here we present some selected results for using different values of $\\lambda$.\n",
    "\n",
    "#### TD(0) for value function, TD(0) for δ- and δ^2-returns\n",
    "\n",
    "```\n",
    "λ for v_hat:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "λ for δ-return:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "λ for δ^2-return:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "v_π:\n",
    " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
    "v_hat:\n",
    " [-1.7646 -1.6445 -1.5414 -1.4214 -0.5     0.    ]\n",
    "bias:\n",
    " [ 0.0005 -0.0535 -0.0098  0.1964 -0.     -0.    ]\n",
    "δ-return:\n",
    " [ 0.0005 -0.0535 -0.0098  0.1964 -0.      0.    ]\n",
    "δ^2-return:\n",
    " [ 0.8254  0.6844  0.4959  0.1904  0.25    0.    ]\n",
    "Approximate δ-return:\n",
    " [-0. -0.  0.  0. -0.  0.]\n",
    "Approximate δ^2-return:\n",
    " [ 0.5659  0.4482  0.3481  0.2304  0.25    0.    ]\n",
    "```\n",
    "\n",
    "The δ-return is not approximable in this case, as expected.\n",
    "\n",
    "The δ^2-return is not particularly well approximated in this case.\n",
    "It appears that the errors in the early states are higher because they incorporate both the inaccuracies in the approximate value function *and* bootstrap from states which have errors themselves.\n",
    "\n",
    "#### TD(1) for value function, TD(1) for δ- and δ^2-returns\n",
    "\n",
    "```\n",
    "λ for v_hat:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "λ for δ-return:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "λ for δ^2-return:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "v_π:\n",
    " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
    "v_hat:\n",
    " [-1.7815 -1.6634 -1.4819 -1.3638 -0.5     0.    ]\n",
    "bias:\n",
    " [ 0.0173 -0.0347 -0.0694  0.1388  0.     -0.    ]\n",
    "δ-return:\n",
    " [ 0.0173 -0.0347 -0.0694  0.1388  0.      0.    ]\n",
    "δ^2-return:\n",
    " [ 0.8175  0.6323  0.4633  0.1711  0.25    0.    ]\n",
    "Approximate δ-return:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "Approximate δ^2-return:\n",
    " [ 0.8246  0.6181  0.4348  0.2282  0.25    0.    ]\n",
    "```\n",
    "\n",
    "As expected, the δ-return is not approximable in this case where all the parameters are the same.\n",
    "\n",
    "The δ^2-return is approximated rather well in this case.\n",
    "\n",
    "#### TD(0) for value function, TD(1) for δ- and δ^2-returns\n",
    "\n",
    "```\n",
    "λ for v_hat:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "λ for δ-return:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "λ for δ^2-return:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "v_π:\n",
    " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
    "v_hat:\n",
    " [-1.7646 -1.6445 -1.5414 -1.4214 -0.5     0.    ]\n",
    "bias:\n",
    " [ 0.0005 -0.0535 -0.0098  0.1964 -0.     -0.    ]\n",
    "δ-return:\n",
    " [ 0.0005 -0.0535 -0.0098  0.1964 -0.      0.    ]\n",
    "δ^2-return:\n",
    " [ 0.8254  0.6844  0.4959  0.1904  0.25    0.    ]\n",
    "Approximate δ-return:\n",
    " [-0.0169 -0.0188  0.0596  0.0576 -0.      0.    ]\n",
    "Approximate δ^2-return:\n",
    " [ 0.8364  0.6625  0.4521  0.2781  0.25    0.    ]\n",
    "```\n",
    "\n",
    "The approximation for the δ-return is nonzero, but it's not particularly robust. \n",
    "\n",
    "The δ^2-return is approximated rather well.\n",
    "\n",
    "#### TD(1) for value function, TD(0) for δ- and δ^2-returns\n",
    "\n",
    "```\n",
    "λ for v_hat:\n",
    " [ 1.  1.  1.  1.  1.  1.]\n",
    "λ for δ-return:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "λ for δ^2-return:\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    "v_π:\n",
    " [-1.7641 -1.6981 -1.5513 -1.225  -0.5    -0.    ]\n",
    "v_hat:\n",
    " [-1.7815 -1.6634 -1.4819 -1.3638 -0.5     0.    ]\n",
    "bias:\n",
    " [ 0.0173 -0.0347 -0.0694  0.1388  0.     -0.    ]\n",
    "δ-return:\n",
    " [ 0.0173 -0.0347 -0.0694  0.1388  0.      0.    ]\n",
    "δ^2-return:\n",
    " [ 0.8175  0.6323  0.4633  0.1711  0.25    0.    ]\n",
    "Approximate δ-return:\n",
    " [ 0.0169  0.0188 -0.0596 -0.0576  0.      0.    ]\n",
    "Approximate δ^2-return:\n",
    " [ 0.5752  0.417   0.3387  0.1805  0.25    0.    ]\n",
    "```\n",
    "\n",
    "The δ-return is approximable, and apparently it's not too far off from the true δ-return.\n",
    "More analysis would be necessary to say whether this is the case in general.\n",
    "\n",
    "The δ^2-return is not well approximated, as might be expected given that TD(0) relies on having a good estimate of subsequent states in order to bootstrap properly.\n",
    "Interestingly, the approximate value is very similar to the approximation for the δ^2-return with TD(0) for the value function and TD(0) for the δ^2-return listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Computations With Approximate Value Function\n",
    "\n",
    "We have some unfinished work examining whether it's either possible or useful to compute the variance with an approximate value function in lieu of $v_{\\pi}$.\n",
    "\n",
    "From a cursory examination that does not appear to be the case, because we can easily end up with negative values and rather large deviations from the 'true' variance of the return.\n",
    "\n",
    "What this means is unclear at the present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TD(λ)\n",
    "w_hat = pinv(X.T @ D @ pinv(I - P @ G @ L) @ (I - P @ G) @ X) @ X.T @ D @ pinv(I - P @ G @ L) @ r\n",
    "v_hat = X @ w_hat\n",
    "\n",
    "# Least-squares\n",
    "# w_hat = w_lsq\n",
    "# v_hat = X @ w_hat\n",
    "\n",
    "# Using the VTD paper to calculate second moments of the return.\n",
    "Pbar = np.zeros((ns, ns))\n",
    "Rbar = np.zeros((ns,ns))\n",
    "rbar = np.zeros(ns)\n",
    "\n",
    "# Specify parameters\n",
    "lvec = np.ones(ns)\n",
    "# lvec = np.zeros(ns)\n",
    "\n",
    "# Calculate R-bar transition matrix\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Rbar[i,j] = R[i,j]**2 + 2*gvec[j]*lvec[j]*R[i,j]*v_hat[j]\n",
    "\n",
    "# Calculate r-bar vector\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        rbar[i] += P[i,j]*Rbar[i,j]\n",
    "\n",
    "# Calculate P-bar\n",
    "for i in range(ns):\n",
    "    for j in range(ns):\n",
    "        Pbar[i,j] += P[i,j]*(gvec[j]**2)*(lvec[j]**2)\n",
    "        \n",
    "        \n",
    "# Calculate second moment of return\n",
    "r_second = pinv(I - Pbar) @ rbar\n",
    "\n",
    "# Variance using Sobel's method and approximate value function\n",
    "T_hat = (P * (R + G @ v_hat)**2) @ np.ones(ns) - v_hat**2\n",
    "        \n",
    "# Solve Bellman equation for variance of return\n",
    "var_hat = pinv(I - P @ G @ G) @ T_hat\n",
    "\n",
    "# Print the results\n",
    "print(\"Second moment of return:\\n\", r_second)\n",
    "print(\"Estimated variance via second moment of return:\\n\", r_second - v_hat**2)\n",
    "print(\"Sobel variance (using v_hat):\\n\", var_hat)\n",
    "print(\"Sobel variance:\\n\", v_var)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
