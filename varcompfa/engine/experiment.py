"""
Code for running experiments in a reproducible way.
"""
import datetime
import logging
import os
import sys
import time
import numpy as np

import varcompfa as vcf

logger = logging.getLogger(__name__)


def _time_string(fmt=None):
    """Get a string representation of the current time, with the option to
    specify a different format."""
    if fmt is None:
        fmt = "%Y-%m-%d-%H-%M-%S-%f"
    return datetime.datetime.now().strftime(fmt)

def _make_experiment_dir(basedir=None, target=None):
    """Make a directory to store data generated by experiment."""
    if basedir is None:
        basedir = os.curdir
    if target is None:
        target = _time_string()
    path = os.path.join(os.path.abspath(basedir), target)
    # os.makedirs(path, exist_ok=True)
    return path


class LiveExperiment:
    """Live experiment class, for running environments with a policy and
    (optionally) learning agents.
    """
    def __init__(self, environment, policy, learners=list()):
        """Create an experiment

        Parameters
        ----------
        env : gym.Env
            The environment on which to run the experiment
        policy : object
            An object that has an `act` function, which produces a valid action
            for the environment given an observation.
        learners: sequence
            A sequence of learning agents to update at each timestep.
        """
        self.env = environment
        self.policy = policy
        self._learners = tuple(learners)

    def run(self, num_episodes, max_steps=100000, callbacks=list(),
            initial_states=None, warn_step_limit=True):
        """Run an experiment.

        Recording the results of the experiment can be done via `Callback`
        classes, which get called at certain times throughout the run.
        These classes pass `dict` objects containing information to the
        callbacks.

        Parameters
        ----------
        num_episodes: int
            The number of episodes to run the experiment for.
        max_steps: int, default=100000
            The maximum number of steps allowed in an episode.
        callbacks: list
            A list of callbacks, objects that may perform actions at certain
            phases of the experiment's execution. (See `varcompfa.callbacks`)
            A single callback object can have different methods which get
            called at different phases of the run's execution.
        initial_states: iterable, optional
            An iterable of initial states to start from, assuming the
            environment supports simply modifying `state`.
            Useful for systematic investigation of policies (e.g. grid search).
        warn_step_limit: boolean, default True
            If `True`, log a warning when the simulation exceeds the step limit.


        Available Callbacks
        -------------------

        - `on_experiment_begin()`
            + Called once per-run, at the start of the experiment.
        - `on_experiment_end()`
            + Called once per-run, at the end of the experiment.
        - `on_episode_begin()`
            + Called once-per episode, prior to the start of the episode.
        - `on_experiment_end()`
            + Called once-per episode, at the end of the episode.
        - `on_step_begin()`
            + Called before executing every step of every episode
        - `on_step_end()`
            + Called at the end of every step of every episode
        """
        # Information that should be generally available
        run_params = {
            'environment': self.env,
            'policy': self.policy,
            'learners': self.learners,
        }

        # Start of experiment callbacks
        run_begin_info = {
            **run_params,
            'num_episodes': num_episodes,
            'max_steps': max_steps,
            'version': vcf.utils.current_version(),
            'git_hash': vcf.utils.current_git_hash(),
            'start_time': datetime.datetime.now(),
        }
        for cbk in callbacks:
            cbk.on_experiment_begin(run_begin_info)

        # Track total number of steps
        total_steps = 0
        # Run for `num_episodes`
        for episode_ix in range(num_episodes):
            # Get learning agents ready for start of new episode
            for agent in self.learners:
                agent.start_episode()
            # Start of episode callbacks
            episode_begin_info = {'total_steps' : total_steps}
            for cbk in callbacks:
                cbk.on_episode_begin(episode_ix, episode_begin_info)

            # Reset the environment, get initial observation
            obs = self.env.reset()
            if initial_states is not None:
                obs = self.env.state = next(initial_states)
            # Run for at most `max_steps` iterations
            for step_ix in range(max_steps):
                # Perform callbacks for beginning of step
                step_begin_info = {}
                for cbk in callbacks:
                    cbk.on_step_begin(step_ix, step_begin_info)

                action = self.policy.act(obs)
                obs_p, reward, done, info = self.env.step(action)

                # Get the basic context from the current time step
                ctx = {
                    'total_steps' : total_steps,
                    'obs': obs,
                    'obs_p': obs_p,
                    'a': action,
                    'r': reward,
                    'done': done,
                }

                # Perform learning for each of the agents
                update_contexts = []
                for agent in self.learners:
                    update_contexts.append(agent.update(ctx))

                # Perform callbacks for end of step
                step_end_info = {
                    'context': ctx,
                    'update_contexts': update_contexts,
                }
                for cbk in callbacks:
                    cbk.on_step_end(step_ix, step_end_info)

                # If terminal state reached, exit episode loop
                if done:
                    total_steps += 1
                    break

                # Otherwise, prepare for next iteration
                obs = obs_p
                total_steps += 1


            # We create a mock final step to ensure the experiment's
            # episodic structure is captured and recorded.
            else:
                # Log a warning about non-termination unless told otherwise
                if warn_step_limit:
                    logger.warn('Failed to terminate episode before time limit.')
                    logger.warn('Updating based on fictitious final step.')


                # Perform callbacks for beginning of step
                step_begin_info = {}
                for cbk in callbacks:
                    cbk.on_step_begin(step_ix, step_begin_info)

                # Create part of the 'terminal' context
                action = self.policy.act(obs)
                obs_p, *_ = self.env.step(action)
                ctx = {
                    'total_steps' : total_steps,
                    'obs': obs,
                    'a': action,
                    'obs_p': obs_p,
                    'done': True,
                    'r': 0,
                }

                # Perform learning for each of the agents
                update_contexts = []
                for agent in self.learners:
                    term_ctx = {**ctx, **agent.terminal_context(ctx)}
                    update_res = agent.update(term_ctx, check_conflict=False)
                    update_contexts.append(update_res)

                # Perform callbacks for end of step
                step_end_info = {
                    'context': ctx,
                    'update_contexts': update_contexts,
                }
                for cbk in callbacks:
                    cbk.on_step_end(step_ix, step_end_info)

            # End of episode, either due to terminating or running out of steps
            # Perform end of episode callbacks
            episode_end_info = {
                'total_steps' : total_steps,
                'context': ctx,
            }
            for cbk in callbacks:
                cbk.on_episode_end(episode_ix, episode_end_info)

        # Perform end of experiment callbacks
        experiment_end_info = {
            'total_steps' : total_steps,
            'end_time': datetime.datetime.now(),
        }
        for cbk in callbacks:
            cbk.on_experiment_end(experiment_end_info)

    @property
    def learners(self):
        """The agents that get updated during each step of the experiment.

        Note
        ----
        Implementing this as a property is a concession towards making the
        tuple of learners more difficult to alter, since doing so could affect
        code that relies on their ordering.
        """
        return self._learners


class ReplayExperiment:
    """Replay experiment class, for replaying and learning from trajectories."""
    def __init__(self, history, learners=list()):
        """Create an experiment

        Parameters
        ----------
        history: dict
            A dictionary with two keys:
                - "contexts": a list of contexts describing each timestep
                - "metadata" metadata about the experiment (some of which may
                get overriden)
        learners: sequence of learning agents
            A sequence of learning agents to update at each timestep.
        """
        self.contexts = history['contexts']
        self._metadata = history['metadata']
        self._learners = tuple(learners)

    def run(self, callbacks=list(), max_episodes=None, max_steps=None):
        """Run an experiment.

        Recording the results of the experiment can be done via `Callback`
        classes, which get called at certain times throughout the run.
        These classes pass `dict` objects containing information to the
        callbacks.

        Parameters
        ----------
        callbacks: list
            A list of callbacks, objects that may perform actions at certain
            phases of the experiment's execution. (See `varcompfa.callbacks`)
            A single callback object can have different methods which get
            called at different phases of the run's execution.
        max_episodes: int, optional
            The number of episodes to run the experiment for.
        max_steps: int, optional
            The maximum number of steps allowed in an episode.

        Available Callbacks
        -------------------

        - `on_experiment_begin()`
            + Called once per-run, at the start of the experiment.
        - `on_experiment_end()`
            + Called once per-run, at the end of the experiment.
        - `on_episode_begin()`
            + Called once-per episode, prior to the start of the episode.
        - `on_experiment_end()`
            + Called once-per episode, at the end of the episode.
        - `on_step_begin()`
            + Called before executing every step of every episode
        - `on_step_end()`
            + Called at the end of every step of every episode
        """
        if max_episodes is None:
            max_episodes = sys.maxsize
        if max_steps is None:
            max_steps = sys.maxsize

        # Start of experiment callbacks
        run_begin_info = {
            **self._metadata,
            'max_episodes': max_episodes,
            'max_steps': max_steps,
            'version': vcf.utils.current_version(),
            'git_hash': vcf.utils.current_git_hash(),
            'start_time': datetime.datetime.now(),
        }
        for cbk in callbacks:
            cbk.on_experiment_begin(run_begin_info)

        # Track total number of steps
        total_steps = 0
        current_episode = 0
        step_ix = 0
        episode_start = True

        contexts = iter(self.contexts)
        for ctx in iter(self.contexts):
            # Run for `max_episodes` or `max_steps`
            if not (current_episode < max_episodes and total_steps < max_steps):
                break

            # Handle start of episode
            if episode_start:
                # Get learning agents ready for start of new episode
                for agent in self.learners:
                    agent.start_episode()

                # Start of episode callbacks
                episode_begin_info = {'total_steps' : total_steps}
                for cbk in callbacks:
                    cbk.on_episode_begin(current_episode, episode_begin_info)

            # Perform callbacks for beginning of step
            step_begin_info = {}
            for cbk in callbacks:
                cbk.on_step_begin(step_ix, step_begin_info)

            # Get the next context
            ctx = next(contexts)

            # Perform learning for each of the agents
            update_contexts = []
            for agent in self.learners:
                update_contexts.append(agent.update(ctx))

            # Perform callbacks for end of step
            step_end_info = {
                'context': ctx,
                'update_contexts': update_contexts,
            }
            for cbk in callbacks:
                cbk.on_step_end(step_ix, step_end_info)

            # Prepare for next iteration
            total_steps += 1
            if ctx['done']:
                # Perform end of episode callbacks
                episode_end_info = {'total_steps' : total_steps, 'context': ctx,}
                for cbk in callbacks:
                    cbk.on_episode_end(current_episode, episode_end_info)

                # Reset step_ix and increment current_episode
                step_ix = 0
                current_episode += 1
                episode_start = True
            else:
                step_ix += 1

        # Perform end of experiment callbacks
        experiment_end_info = {
            'total_steps' : total_steps,
            'end_time': datetime.datetime.now(),
        }
        for cbk in callbacks:
            cbk.on_experiment_end(experiment_end_info)

    @property
    def learners(self):
        """The agents that get updated during each step of the experiment.

        Note
        ----
        Implementing this as a property is a concession towards making the
        tuple of learners more difficult to alter, since doing so could affect
        code that relies on their ordering.
        """
        return self._learners
